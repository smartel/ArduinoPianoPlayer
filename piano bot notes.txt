Arduino Piano Player overview:




The end goal is to have an arduino, with robotic fingers that slide along a track, be able to quickly process and play a piece of music from potentially many different standard formats 
	(such as an unedited MusicXML file, reminiscent of sheet music), on a physical keyboard or piano.
	Ideally, it will be able to play music I'm not personally capable of playing.



	
High-level overview of the process:

Multiple different steps will take us from having a source file in some standard format (such as MusicXML), and end up with the arduino playing the music on a physical piano.
A program that translates from potentially multiple different standard formats (like musicxml) into my own custom format (an .alc file, Alchemized Music Data File)
A program that processes my format, and turns it into another proprietary format, "finger instructions" (a ".fng", finger file) - commands that are sent to the arduino, telling it how to move its "fingers" and for how long, to play the music piece.
	The finger instructions may potentially end up being code written by code - possibly exporting raw arduino commands that are just run directly, meaning no "player program" may actually exist outside of the exported code.
	This exported code will need to be based on the individual piano's properties (how many keys / octaves it has) as well as how many fingers are available for the arduino to instruct.

To further clarify - a song will not be played live just by importing an xml file. Chords and "finger" movements will need to be mapped out, so this will be at least a 2 step process:
	1. import standard file (or my .alc file format)
	2. export raw arduino code to physical device (instructions are exported in my .fng file format)
	
Additional utilities, such as a "Song stats analyzer" (name TBD), may be developed to determine local maximums / keys that are hit most often in a song,
	as well as the lowest tone hit, highest tone hit, and so on, so we can optimize "finger" placement for individual songs.

	


Technical information:

The yamaha dgx205 keyboard I am intending to try the arduino piano player on contains 45 white keys and 31 black keys, for a total of 76 keys. Its first key is an E. Its last key is a G.

Alchemized music data file implementation:
	Header line, which will be checked to confirm it is an .alc file, as well as containing some basic information like song name (TBD)
Tentative: my music file format will, at a minimum, need to know for each note:
		TIMESTAMP IN SONG (likely in milliseconds):
		NOTE:
		OCTAVE: (include this in the note itself?)
		WHETHER IT IS FLAT OR SHARP: (we don't care about the key, so there must always be a sharp or flat indicator present if the note is sharp or flat)
		NOTE DURATION: (note, some amount may be subtracted from this, so if it was 1 second, it may be 0.9 so it can have time to get to the next note)
						we may also need to store bpm, whether it is a quarter note, half note, whole note, and so on?
							and we may have to reject notes that are too small, like 1/8th, 1/16th, 1/32th ... notes.
When processed, instructions will be assigned to these notes and given a specific finger to hit it.
The instructions can then be reduced to specific commands for each finger, for example:
Finger 1: wait x milliseconds, slide y direction for z time, depress white/black key for duration, retract finger, wait a, slide b direction for c time, depress white/black key for duration, ...
Finger 2: "" with different waits, durations, slide time, and so on.
Finger n: ""
Presumably all of these instructions will need to be merged into one linear instruction file / order of operations.
While my file format will have unedited durations, the durations may need to be shrunk slightly in the actual finger instructions, to account for time it takes to retract/depress keys and slide.
	See: unanswered questions section.

	
	
	
Project name: PianoPlayer
Project terminology:

Translate - converting from one standard file format (such as MusicXML) to my file format (.alc, Alchemized Music Data File)
Transpose - converting from my file format (.alc) to the raw instructions (.fng file) sent to the arduino to play the piano

Lost Time - not sure if this will be a real issue yet. But if a finger has to stop depressing a note early, to move to its next note to hit (say, .10 seconds early),
	        we'll consider that .10 seconds of "lost (play)time"

.alc file format - My alchemized music data file, which is how my program processes music note information. This exists so I only need to process one format.
	"Translators" will exist to convert other standard formats (like MusicXML) into my format
.fng file format - My "finger instruction" file, that contains the commands to be sent to the arduino to play the music piece.
	
TRANSPOSER:
	MusicNote - an individual note from a song / sheet music / what have you. Contains at least the note, octave, and duration.
	MusicSlice - A collection of all MusicNotes that need to be struck at one very specific period of time
	MusicSheet - a collection of slices (thus, ultimately a collection of all music notes in the song and when they need to be hit)
	FingerAssigner - this knows the positions of all the fingers, and processes a music sheet, assigning instructions to fingers.
	FingerInstruction - an individual "message" telling a single finger to move to a given note and to press it for a given duration
FEIGNER:
	A short term basic java gui that shows the position of fingers on a keyboard, and attempts to replicate how the arduino will move fingers and play a song given an .fng file.
RASPBERRY FOX:
	The name of my physical piano-playing arduino.
	
	
	

Remaining questions:

Whoops, I think we do need to init the starting octave and not just always assume it will be 1. Because the musicxml sets the octave values. You will need to determine it on a per-piano basis.

What other formats should we consider being able to import?
	MIDI - other programs out there can convert midi to text file, so we could work from one of those?

How many fingers will the physical hardware have?
	Keeping in mind humans only have ten fingers, albeit some advanced chords call for moves like having the thumb depress 2 notes at the same time.
	To have a decent range, we probably want to consider having at least 3 fingers for the arduino's "left hand" and 3 for its "right hand", meaning 6 total at a minimum.

Hey, maybe we could do some minor arduino testing with LED's?
Maybe we'll put some LED's on the actual fingers too?
	
Do we need fingers to let go of their key slightly early, to quickly move to the next key it needs to hit?
For example: given there is only ONE FINGER in this example (or, highly situational - no other fingers will be able to reach the key in time due to their positions / being blocked by other fingers),
	         and the one finger is hitting an arbitrary key (let's say a quarter note), and then needs to move and hit an adjacent note next to it (another quarter note).
			 Do we need to plan for the time it will take to stop hitting its current note, and move to the next note, into its instructions?
			 For example, maybe it will take 0.05 seconds to retract a key, and a baseline 0.10 seconds to slide over to an adjacent key
				(plus an additional let's say ~0.5 seconds per additional key it needs to slide pass)
			 WHAT WOULD THE SOLUTION BE: do we want to stop depressing its current note ~0.15 seconds early,
										 do we cut the difference in half between the two notes? (undepress 0.075 early and depress the correct note 0.075 late).
			 We'll likely need to play this by ear to figure out what sounds best. Ideally with enough fingers it won't be an issue.
If nothing else, we should be able to document constants, such as:
	how long it takes to retract a key press before it can safely slide to a new note,
	how long it takes baseline to slide across the physical gap to an adjacent note,
	how long it will take to slide for each additional note (presumably the same amount of time as just sliding to an adjacent note, multiplied?),
	how long it will take to depress the new note,
	how long it takes to hit a sharp / flat note versus a white note (if it takes a different amount of time to depress or retract these keys)
This may end up being: Retract time + Depress time + (Gap distance in notes * Time to travel past an individual note) +/- sharp or flat modification =
	Final time to subtract from playing a note or split across the previous and next note?
Optimal solutions should aim to have as few fingers as possible utilized, WHILE ALSO HAVING AS LITTLE "LOST TIME" AS POSSIBLE (ie, if you lift off a note .10 seconds early, that will be 0.10 seconds "lost time")

How do we want to initial finger placement before a song starts / at the start of a song?
	Do we have all fingers move as far to the left as possible before a song starts or after a song ends?
		This way, we'll always know their position, as in, the first finger is on the first key, the second finger is on the second key, and so on.
	Do we want to take a few seconds to "initialize" before starting to play the song, where the fingers move from their starting positions to the first notes each will need to hit,
		but wait until they are all stationed before starting to play the song?
			There could be more fingers than keys that need to be hit at any given time, so do we want to strategically map out local maximums / which keys are hit most often, to position fingers there?

While collissions are not intended, it may be best to test what would happen if we had two fingers try to cross past each other,
	as well as how gracefully two fingers can be next to each other / hit directly adjacent notes.
	We need to know early on if we need to account for a gap between fingers, which would be terrible.
	So realistically, we need to know if we need to make "skinnier" fingers, or some sort of alternating physical shape that allows fingers to directly neighbor with each other safely.
		Will there be room for a note to be hit simultaneously with its flat / sharp? That might be too close. How common is that in actual songs? Would that be dissonant?
			Doesn't matter if it's dissonant, if it is what is in the actual unedited MusicXML. Either find a way to play it or list it as a limitation.

Do we want to make our own "Finger Reducer" in the Transposer? Like, have it try to calculate the fewest fingers you'd need to do the song?
	While not my proudest idea, we could use the "problem solver framework" from years and years ago to effectively try to brute-force find solutions using the least number of fingers.
		That is, if we have a solution that uses 6 fingers, you could then run the brute-force using 5 and see if it returns anything, and then using 4, and so on, til it can't find a solution.
			ONE OF THE EASIEST CHECKS YOU CAN DO IS "What is the maximum number of keys that are hit simultaneously in this song?" to find out AT LEAST how many fingers you'll need.
	
We'll want to figure out what is "humanly possible" (machine-ly possible?) for the arduino to handle, ie, how fast can it hit notes / arpeggios / "do hammer ons", how fast can it move across octaves, and so on.
	Should first start by learning how much is possible / what are the constraints on a single "finger", before adding more fingers.
	Do we care how softly / hard it can hit notes? Not out of physical worry for the keyboard (although we shouldn't intentionally be rough), but to create softer / harder sounds? Or we can just turn the "touch" option off on my yamaha.

Best way to determine and store "duration" ? Since we need to worry about bpm / time signatures
	We may want to store the unadultered duration in a note object itself, and then the adjusted duration (if we need it to lift off the key 0.05 seconds early or something) in an "instruction" object
	Ok, so, we can determine how long a beat is, or a measure is, on a song-per-song basis, by looking at their bpm?
		So we read in a song, we see idk 80 bpm, determine a beat is (lets pretend) 1 second, we store that 1 second, and pull it whenever we need to know how long a quarter note / half note / whole note etc is
			And we'll store it in seconds or milliseconds, so like, perhaps we'll determine and store it as 950ms

I'd hate to mark up the piano, but if we need to orient it, we could place a sticker or a bright piece of paper on a key (first key? some middle c?), and have a camera at one end of the track,
	in case it needs to do some like, micro adjustments. if it came down to having bright paper on every key, id be sad. should be able to just get an accurate width of key gaps and how much motor power to slide across key(s).

A very, very worst case scenario, if you can't find a way to have a motor move fingers across a track, would be to put... 76 individual fingers, one for each key : )
	While that isn't the goal of my project, you could probably get some crazy arpeggio's / some crazy ridiculous songs to work since there wouldn't be any sliding needed.

Will it be able to play Recollection? I could never do the last 45 seconds.




Perfectionism / Future:

MusicXML translating:
	Issue 1:
		Wanted to have a rule to never modify the xml files, but some musicxml DTD's give me a 403 error, and then modifying the xml by hand to change to a different DTD causes some entities to fail to resolve, such as "&auml" in <text> elements.
			org.xml.sax.SAXParseException; systemId: file:/// REDACTED; lineNumber: 41; columnNumber: 40; The entity "auml" was referenced, but not declared.
		Ultimately I just end up removing both the reference to the 403 DTD and the &___ entities that fail to resolve.
	Issue 2:
		Sometimes the musicxml contains parts we don't want to convert, such as multiple instruments that aren't piano. While we could leave them in and convert the pitches to piano keys, these non-piano parts are another thing I delete from the files manually sometimes.
	Issue 3:
		How to handle multiple voices, when the other voices aren't piano? Have been manually deleting unwanted voices from the xml. There are a few different options:
			1. Export each voice to an individual .alc file, with the name of the voice / instrument included in the file name?
			2. Merge all the voices into one .alc file anyway? May be very dissonant / messy merging different instruments into one piano track.
			3. Specify which voice(s) you want to keep as an optional input parameter. I'm not positive but I think voices can be named anything, so it wouldn't be as simple as just hardcoding to target "Piano", hence optional input parameter(s)
			
			